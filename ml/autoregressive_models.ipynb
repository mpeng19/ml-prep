{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GPT-2 architecture and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attn_masks = tokenized_data[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attn_masks[idx], dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    n_positions = 512,\n",
    "    n_ctx = 512,\n",
    "    n_embd = 768,\n",
    "    n_layer = 6,\n",
    "    n_head = 12,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model - takes too long bc I don't have gpus\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{epochs}\", unit = \"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(train_dataloader)\n",
    "print(f\"Epoch {epoch+1} loss: {avg_loss:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained GPT2 and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Output Field** | **When Available**                                      | **Shape / Content**                          |\n",
    "|------------------|----------------------------------------------------------|----------------------------------------------|\n",
    "| `logits`         | Always                                                   | `(batch_size, seq_len, vocab_size)`          |\n",
    "| `hidden_states`  | `output_hidden_states=True`                             | Tuple of `(batch_size, seq_len, hidden_size)`|\n",
    "| `attentions`     | `output_attentions=True`                                | Tuple of `(batch_size, num_heads, seq_len, seq_len)` |\n",
    "| `scores` (in `.generate()`) | `output_scores=True` & `return_dict_in_generate=True` | Logits at each generation step        |\n",
    "| `sequences`      | Always with `.generate()`                               | Final generated token IDs                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct model call to .generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the meaning of life is to bring back a feeling of the past into your mind with an emotion like pain or anguish. It\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = \"the meaning of life is\"\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#use model's generate function to automatically perform sampling\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is still in its infancy, but it is already a promising technology to take advantage and eventually disrupt our own. The same goes for the future of financial services and cyber security, both of which are rapidly changing in importance. These innovations will provide us with a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "example_input = \"The future of AI is\"\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(input_ids=generated_ids, attention_mask=attention_mask) #just call the model and get output logits\n",
    "        #outputs[1] is past key/value (kv cache)\n",
    "        #outputs[2] is hidden_state\n",
    "        #outputs[3] is attention values\n",
    "        #ONLY IF USE_CACHE=TRUE (default is true)\n",
    "\n",
    "        # print(outputs[0].shape) #(batch_size, seq_len, vocab_size), for every batch, for each elt, returns the logits of every token\n",
    "        # check to make sure that it sums to 1\n",
    "        '''\n",
    "        # probs = torch.softmax(outputs[0], dim=-1)\n",
    "        # print(torch.sum(probs[0], dim = -1))\n",
    "        '''\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]  #(1, vocab_size)\n",
    "\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        topk_logits, topk_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "        probs = torch.softmax(topk_logits, dim=-1)\n",
    "\n",
    "        sampled_index = torch.multinomial(probs, num_samples=1)  #(1, 1)\n",
    "        next_token_id = topk_indices.gather(-1, sampled_index)   #map back to full vocab ID, gather the sampled index\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=-1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no sampling but custom forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one plus two equals one.\n",
      "\n",
      "The first is the \"one-two\" rule. The second is the \"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = \"one plus two equals\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device, dtype=torch.bool)\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "all_hidden_states = []\n",
    "max_new_tokens=20\n",
    "config=model.config\n",
    "\n",
    "def forward(input_ids, attention_mask):\n",
    "  hidden_states = []\n",
    "\n",
    "  embeddings = model.transformer.wte(input_ids) + model.transformer.wpe(torch.arange(input_ids.size(1), device = device))\n",
    "  hidden_state = embeddings #(batch_size, seq_len, n_embd)\n",
    "  hidden_states.append(hidden_state.cpu().detach().numpy()) #store initial embedding\n",
    "\n",
    "  seq_len=input_ids.size(1)\n",
    "  causal_mask=torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "  attention_mask=attention_mask.unsqueeze(1).unsqueeze(2) & causal_mask\n",
    "\n",
    "  for layer_idx in range(config.n_layer):\n",
    "    layer = model.transformer.h[layer_idx]\n",
    "    outputs = layer(hidden_state, attention_mask = attention_mask)\n",
    "    hidden_state = outputs[0] #update hidden state\n",
    "    hidden_states.append(hidden_state.cpu().detach().numpy())\n",
    "\n",
    "  hidden_state = model.transformer.ln_f(hidden_state)\n",
    "  hidden_states.append(hidden_state.cpu().detach().numpy())\n",
    "\n",
    "  logits = model.lm_head(hidden_state)\n",
    "  return logits, hidden_states\n",
    "\n",
    "with torch.no_grad():\n",
    "  for _ in range(max_new_tokens):\n",
    "    logits, hidden_states = forward(generated_ids, attention_mask)\n",
    "    all_hidden_states.append(hidden_states)\n",
    "\n",
    "    next_token_logits = logits[:, -1, :] #(1, vocab_size)\n",
    "    next_token_id = torch.argmax(next_token_logits, dim = -1) #get the token with the highest probability, dim = 1) - size of [1]\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(-1)], dim = -1)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id.unsqueeze(-1), dtype=torch.bool)], dim = -1)\n",
    "\n",
    "    if next_token_id.item() == tokenizer.eos_token_id:\n",
    "      break\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom implementation of Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #conv functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, num_heads):\n",
    "    super().__init__()\n",
    "    assert hidden_size%num_heads==0 #we must be able to split the input vector evenly amonst the heads\n",
    "    self.hidden_size=hidden_size\n",
    "    self.num_heads=num_heads\n",
    "    self.head_dim=hidden_size//num_heads\n",
    "\n",
    "    #we use a linear layer for q,k,v in order to learn projections Q = self.query(x) -> learn information about x\n",
    "    self.query=nn.Linear(in_features = hidden_size, out_features = hidden_size)\n",
    "    self.key=nn.Linear(in_features = hidden_size, out_features = hidden_size)\n",
    "    self.value=nn.Linear(in_feautures = hidden_size, out_features = hidden_size)\n",
    "\n",
    "    self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "    self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "  def forward(self, query, key, value, mask = None):\n",
    "    batch_size = query.size(0)\n",
    "\n",
    "    #linear transformations - these are all (batch_size, sequence_length, hidden_size)\n",
    "    Q=self.query(query)\n",
    "    K=self.query(key)\n",
    "    V=self.query(value)\n",
    "\n",
    "    #split into heads\n",
    "    #(bs, num_heads, seq_len, head_dim); tranpose is so we can group by heads for parallelization\n",
    "    Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "    K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    #scaled dot product attention\n",
    "    #(bs, num_heads, seq_len, head_dim) * (bs, num_heads, head_dim, seq_len) -> (bs, num_heads, seq_len, seq_len) = qk matmul for every pair of inputs (including with itself)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "    if mask is not None:\n",
    "      scores = scores.masked_fill(mask == False, float('-inf'))\n",
    "\n",
    "    attn_weights=F.softmax(scores, dim=-1) #(bs, num_heads, seq_len, seq_len)\n",
    "    attn_output=torch.matmul(attn_weights, V) #(bs, num_heads, seq_len, seq_len) * (bs, num_heads, seq_len, head_dim) -> (bs, num_heads, seq_len, head_dim)\n",
    "\n",
    "    #concatenate heads\n",
    "    #first transpose back to (bs, seq_len, num_heads, head_dim) -> so we're grouping by tokens and we can see all heads per token embedding\n",
    "    attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.hidden_size) #let pytorch figure out that the middle dim is seq_len using -1\n",
    "    output=self.fc_out(attn_output) #(bs, seq_len, hidden_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.dropout(self.embedding(input_ids))  #(batch_size, seq_len, embed_size)\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(embedded)  #(batch_size, seq_len, hidden_size)\n",
    "        attn_output, _ = self.self_attention(lstm_outputs, lstm_outputs, lstm_outputs)  #(batch_size, seq_len, hidden_size)\n",
    "        outputs = self.norm(lstm_outputs + attn_output)  #residuals\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden, cell, encoder_outputs, self_attn_mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_ids))  #(batch_size, seq_len, embed_size)\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))  #(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        #Masked self-attention\n",
    "        self_attn_output, _ = self.self_attention(lstm_outputs, lstm_outputs, lstm_outputs, self_attn_mask)\n",
    "        self_attn_output = self.norm1(lstm_outputs + self_attn_output)\n",
    "\n",
    "        #Cross-attention with encoder outputs\n",
    "        cross_attn_output, _ = self.cross_attention(self_attn_output, encoder_outputs, encoder_outputs)\n",
    "        outputs = self.norm2(self_attn_output + cross_attn_output)\n",
    "\n",
    "        predictions = self.fc(outputs.squeeze(1) if input_ids.size(1) == 1 else outputs)  #(batch_size, vocab_size) or (batch_size, seq_len, vocab_size)\n",
    "        return predictions, hidden, cell, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderLSTM(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(EncoderDecoderLSTM, self).__init__()\n",
    "        self.encoder = EncoderLSTM(src_vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.decoder = DecoderLSTM(tgt_vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, teacher_forcing_ratio=0.5):\n",
    "        #src_ids: (batch_size, src_seq_len)\n",
    "        #tgt_ids: (batch_size, tgt_seq_len)\n",
    "        batch_size = src_ids.size(0)\n",
    "        tgt_seq_len = tgt_ids.size(1)\n",
    "        device = src_ids.device\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_ids)\n",
    "\n",
    "        self_attn_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=device)).bool()\n",
    "        self_attn_mask = self_attn_mask.unsqueeze(0).unsqueeze(1)  #(1, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_seq_len, self.tgt_vocab_size).to(device)\n",
    "        input_id = tgt_ids[:, 0].unsqueeze(1)  #(batch_size, 1)\n",
    "        decoder_hidden = torch.zeros_like(encoder_outputs)\n",
    "\n",
    "        for t in range(1, tgt_seq_len):\n",
    "            output, hidden, cell, decoder_hidden = self.decoder(\n",
    "                input_id, hidden, cell, encoder_outputs,\n",
    "                self_attn_mask[:, :, :t+1, :t+1] if t < tgt_seq_len-1 else self_attn_mask\n",
    "            )\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_id = tgt_ids[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, src_ids, max_len=50, sos_token=1, eos_token=2):\n",
    "        batch_size = src_ids.size(0)\n",
    "        device = src_ids.device\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_ids)\n",
    "\n",
    "        generated_ids = torch.ones(batch_size, 1, dtype=torch.long).to(device) * sos_token\n",
    "        decoder_hidden = torch.zeros_like(encoder_outputs)\n",
    "\n",
    "        input_id = generated_ids[:, -1].unsqueeze(1)\n",
    "        self_attn_mask = torch.tril(torch.ones((max_len, max_len), device=device)).bool()\n",
    "        self_attn_mask = self_attn_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            output, hidden, cell, decoder_hidden = self.decoder(\n",
    "                input_id, hidden, cell, encoder_outputs,\n",
    "                self_attn_mask[:, :, :t+2, :t+2] if t < max_len-1 else self_attn_mask\n",
    "            )\n",
    "            next_token = output.argmax(1).unsqueeze(1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "            input_id = next_token\n",
    "            if (next_token == eos_token).all():\n",
    "                break\n",
    "\n",
    "        return generated_ids"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
