{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhcG2FVUXstO"
   },
   "source": [
    "#Load GPT-2 architecture and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJF2UxA1KUoJ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets==2.14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AprwuqzP83Zm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCzXiRahJrfT"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attn_masks = tokenized_data[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attn_masks[idx], dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "68d8705099ed4ebaa86d154c02d32916",
      "56a56e78c60446ae99677c36d80b2b4c",
      "84aae215b4bc4d20abdf8a0f2d6a543f",
      "273fe7f8c42b4aa29bdc29c8636b3797",
      "9f9cc379e9ef4eca943e509757695662",
      "da8f6cc646f04c349b1744c770bf01ef",
      "a47399e92d3e434193e60eae45453845",
      "459fda7704ac44bf817a68edad821662",
      "37e46a4a9eca43cf87ab9f8281deb9c0",
      "93f1bbe722ce4b7aa6da21eefdac502c",
      "b9595532688548e3a10b40c712d8f0d7",
      "5a31a52c17234878a5cfe017b1b1fe2c",
      "8f5a9e7a78da4c04b8cc5e3c3912d8b2",
      "0564f2cba4a1417ca19d80685a961fb6",
      "2a88af8ea15b4dd69a5ec53725a213cc",
      "229594db80b5495bb57edeb247c51c3b",
      "e603e9110d2e44248f15c98c0f5b50c3",
      "037ef42c503745e8b511c3b67e19c33a",
      "b0724347077b4db191fa46fa2d9c8938",
      "c7ebf122eceb4962b2dd04102d9fce57",
      "e8f05653ad7448b3ae8eabbc0a267445",
      "f8d5c4d3b8744c22b284d22db9347083"
     ]
    },
    "executionInfo": {
     "elapsed": 7632,
     "status": "ok",
     "timestamp": 1747352745660,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "5tIDoHJlJrnS",
    "outputId": "f88723c4-6e47-4643-927a-0d873092e1c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d8705099ed4ebaa86d154c02d32916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a31a52c17234878a5cfe017b1b1fe2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    n_positions = 512,\n",
    "    n_ctx = 512,\n",
    "    n_embd = 768,\n",
    "    n_layer = 6,\n",
    "    n_head = 12,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGcSpK2_Ki4G"
   },
   "outputs": [],
   "source": [
    "#Train model - takes too long bc I don't have gpus\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{epochs}\", unit = \"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(train_dataloader)\n",
    "print(f\"Epoch {epoch+1} loss: {avg_loss:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyh4IftwXyCt"
   },
   "source": [
    "#Load pre-trained GPT2 and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNq5MUCUWx1x"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8cXlmuiZUO8"
   },
   "source": [
    "###GPT-2 Outputs\n",
    "\n",
    "| **Output Field** | **When Available**                                      | **Shape / Content**                          |\n",
    "|------------------|----------------------------------------------------------|----------------------------------------------|\n",
    "| `logits`         | Always                                                   | `(batch_size, seq_len, vocab_size)`          |\n",
    "| `hidden_states`  | `output_hidden_states=True`                             | Tuple of `(batch_size, seq_len, hidden_size)`|\n",
    "| `attentions`     | `output_attentions=True`                                | Tuple of `(batch_size, num_heads, seq_len, seq_len)` |\n",
    "| `scores` (in `.generate()`) | `output_scores=True` & `return_dict_in_generate=True` | Logits at each generation step        |\n",
    "| `sequences`      | Always with `.generate()`                               | Final generated token IDs                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rADs-OLjX3h-"
   },
   "source": [
    "##One way of Running Inference - direct model call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1747354019503,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "ogAeiG-fX3CX",
    "outputId": "9d68cb1f-1c7e-4647-85c5-80052edec309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the meaning of life is more or less universal. One way of interpreting the meaning of life was given by Dr. Alber\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = \"the meaning of life is\"\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#use model's generate function to automatically perform sampling\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr6gNJvyYXQb"
   },
   "source": [
    "##Another way of running inference - sampling only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1747356556762,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "3P6V4M-WYctO",
    "outputId": "f7347bd7-a9d0-41f3-c1c4-be16cc926cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is very uncertain. And we don't know when,\" said Besser, a physicist at the University of Manchester, in a research note, as he looked out the window of his home. \"But we're definitely going to have to see more.\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "example_input = \"The future of AI is\"\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(input_ids=generated_ids, attention_mask=attention_mask) #just call the model and get output logits\n",
    "        #outputs[1] is past key/value (kv cache)\n",
    "        #outputs[2] is hidden_state\n",
    "        #outputs[3] is attention values\n",
    "        #ONLY IF USE_CACHE=TRUE (default is true)\n",
    "\n",
    "        # print(outputs[0].shape) #(batch_size, seq_len, vocab_size), for every batch, for each elt, returns the logits of every token\n",
    "        # check to make sure that it sums to 1\n",
    "        '''\n",
    "        # probs = torch.softmax(outputs[0], dim=-1)\n",
    "        # print(torch.sum(probs[0], dim = -1))\n",
    "        '''\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]  #(1, vocab_size)\n",
    "\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        topk_logits, topk_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "        probs = torch.softmax(topk_logits, dim=-1)\n",
    "\n",
    "        sampled_index = torch.multinomial(probs, num_samples=1)  #(1, 1)\n",
    "        next_token_id = topk_indices.gather(-1, sampled_index)   #map back to full vocab ID, gather the sampled index\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=-1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONAbBpMskU8s"
   },
   "source": [
    "##Third inference method - no sampling but custom forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1747356971910,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "nJjvQ9ixLZyq",
    "outputId": "fe992296-fd72-4149-e46d-2efa6a5d7354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one plus two equals one.\n",
      "\n",
      "The first is the \"one-two\" rule. The second is the \"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = \"one plus two equals\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(example_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device, dtype=torch.bool)\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "all_hidden_states = []\n",
    "max_new_tokens=20\n",
    "config=model.config\n",
    "\n",
    "def forward(input_ids, attention_mask):\n",
    "  hidden_states = []\n",
    "\n",
    "  embeddings = model.transformer.wte(input_ids) + model.transformer.wpe(torch.arange(input_ids.size(1), device = device))\n",
    "  hidden_state = embeddings #(batch_size, seq_len, n_embd)\n",
    "  hidden_states.append(hidden_state.cpu().detach().numpy()) #store initial embedding\n",
    "\n",
    "  seq_len=input_ids.size(1)\n",
    "  causal_mask=torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "  attention_mask=attention_mask.unsqueeze(1).unsqueeze(2) & causal_mask\n",
    "\n",
    "  for layer_idx in range(config.n_layer):\n",
    "    layer = model.transformer.h[layer_idx]\n",
    "    outputs = layer(hidden_state, attention_mask = attention_mask)\n",
    "    hidden_state = outputs[0] #update hidden state\n",
    "    hidden_states.append(hidden_state.cpu().detach().numpy())\n",
    "\n",
    "  hidden_state = model.transformer.ln_f(hidden_state)\n",
    "  hidden_states.append(hidden_state.cpu().detach().numpy())\n",
    "\n",
    "  logits = model.lm_head(hidden_state)\n",
    "  return logits, hidden_states\n",
    "\n",
    "with torch.no_grad():\n",
    "  for _ in range(max_new_tokens):\n",
    "    logits, hidden_states = forward(generated_ids, attention_mask)\n",
    "    all_hidden_states.append(hidden_states)\n",
    "\n",
    "    next_token_logits = logits[:, -1, :] #(1, vocab_size)\n",
    "    next_token_id = torch.argmax(next_token_logits, dim = -1) #get the token with the highest probability, dim = 1) - size of [1]\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(-1)], dim = -1)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id.unsqueeze(-1), dtype=torch.bool)], dim = -1)\n",
    "\n",
    "    if next_token_id.item() == tokenizer.eos_token_id:\n",
    "      break\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7L0h-Qul5Kk"
   },
   "source": [
    "#Custom implementation of Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKgI_1Vel7n7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #conv functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6ky1YMsoZKD"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, num_heads):\n",
    "    super().__init__()\n",
    "    assert hidden_size%num_heads==0 #we must be able to split the input vector evenly amonst the heads\n",
    "    self.hidden_size=hidden_size\n",
    "    self.num_heads=num_heads\n",
    "    self.head_dim=hidden_size//num_heads\n",
    "\n",
    "    #we use a linear layer for q,k,v in order to learn projections Q = self.query(x) -> learn information about x\n",
    "    self.query=nn.Linear(in_features = hidden_size, out_features = hidden_size)\n",
    "    self.key=nn.Linear(in_features = hidden_size, out_features = hidden_size)\n",
    "    self.value=nn.Linear(in_feautures = hidden_size, out_features = hidden_size)\n",
    "\n",
    "    self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "    self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "  def forward(self, query, key, value, mask = None):\n",
    "    batch_size = query.size(0)\n",
    "\n",
    "    #linear transformations - these are all (batch_size, sequence_length, hidden_size)\n",
    "    Q=self.query(query)\n",
    "    K=self.query(key)\n",
    "    V=self.query(value)\n",
    "\n",
    "    #split into heads\n",
    "    #(bs, num_heads, seq_len, head_dim); tranpose is so we can group by heads for parallelization\n",
    "    Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "    K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    #scaled dot product attention\n",
    "    #(bs, num_heads, seq_len, head_dim) * (bs, num_heads, head_dim, seq_len) -> (bs, num_heads, seq_len, seq_len) = qk matmul for every pair of inputs (including with itself)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "    if mask is not None:\n",
    "      scores = scores.masked_fill(mask == False, float('-inf'))\n",
    "\n",
    "    attn_weights=F.softmax(scores, dim=-1) #(bs, num_heads, seq_len, seq_len)\n",
    "    attn_output=torch.matmul(attn_weights, V) #(bs, num_heads, seq_len, seq_len) * (bs, num_heads, seq_len, head_dim) -> (bs, num_heads, seq_len, head_dim)\n",
    "\n",
    "    #concatenate heads\n",
    "    #first transpose back to (bs, seq_len, num_heads, head_dim) -> so we're grouping by tokens and we can see all heads per token embedding\n",
    "    attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.hidden_size) #let pytorch figure out that the middle dim is seq_len using -1\n",
    "    output=self.fc_out(attn_output) #(bs, seq_len, hidden_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nduq3SrUqF35"
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "=        embedded = self.dropout(self.embedding(input_ids))  #(batch_size, seq_len, embed_size)\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(embedded)  #(batch_size, seq_len, hidden_size)\n",
    "        attn_output, _ = self.self_attention(lstm_outputs, lstm_outputs, lstm_outputs)  #(batch_size, seq_len, hidden_size)\n",
    "        outputs = self.norm(lstm_outputs + attn_output)  #residuals\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden, cell, encoder_outputs, self_attn_mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_ids))  #(batch_size, seq_len, embed_size)\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))  #(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        #Masked self-attention\n",
    "        self_attn_output, _ = self.self_attention(lstm_outputs, lstm_outputs, lstm_outputs, self_attn_mask)\n",
    "        self_attn_output = self.norm1(lstm_outputs + self_attn_output)\n",
    "\n",
    "        #Cross-attention with encoder outputs\n",
    "        cross_attn_output, _ = self.cross_attention(self_attn_output, encoder_outputs, encoder_outputs)\n",
    "        outputs = self.norm2(self_attn_output + cross_attn_output)\n",
    "\n",
    "        predictions = self.fc(outputs.squeeze(1) if input_ids.size(1) == 1 else outputs)  #(batch_size, vocab_size) or (batch_size, seq_len, vocab_size)\n",
    "        return predictions, hidden, cell, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evezK5tJCVSD"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderLSTM(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size, num_layers=1, num_heads=8, dropout=0.3):\n",
    "        super(EncoderDecoderLSTM, self).__init__()\n",
    "        self.encoder = EncoderLSTM(src_vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.decoder = DecoderLSTM(tgt_vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, teacher_forcing_ratio=0.5):\n",
    "        #src_ids: (batch_size, src_seq_len)\n",
    "        #tgt_ids: (batch_size, tgt_seq_len)\n",
    "        batch_size = src_ids.size(0)\n",
    "        tgt_seq_len = tgt_ids.size(1)\n",
    "        device = src_ids.device\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_ids)\n",
    "\n",
    "        self_attn_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=device)).bool()\n",
    "        self_attn_mask = self_attn_mask.unsqueeze(0).unsqueeze(1)  #(1, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_seq_len, self.tgt_vocab_size).to(device)\n",
    "        input_id = tgt_ids[:, 0].unsqueeze(1)  #(batch_size, 1)\n",
    "        decoder_hidden = torch.zeros_like(encoder_outputs)\n",
    "\n",
    "        for t in range(1, tgt_seq_len):\n",
    "            output, hidden, cell, decoder_hidden = self.decoder(\n",
    "                input_id, hidden, cell, encoder_outputs,\n",
    "                self_attn_mask[:, :, :t+1, :t+1] if t < tgt_seq_len-1 else self_attn_mask\n",
    "            )\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_id = tgt_ids[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, src_ids, max_len=50, sos_token=1, eos_token=2):\n",
    "        batch_size = src_ids.size(0)\n",
    "        device = src_ids.device\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_ids)\n",
    "\n",
    "        generated_ids = torch.ones(batch_size, 1, dtype=torch.long).to(device) * sos_token\n",
    "        decoder_hidden = torch.zeros_like(encoder_outputs)\n",
    "\n",
    "        input_id = generated_ids[:, -1].unsqueeze(1)\n",
    "        self_attn_mask = torch.tril(torch.ones((max_len, max_len), device=device)).bool()\n",
    "        self_attn_mask = self_attn_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            output, hidden, cell, decoder_hidden = self.decoder(\n",
    "                input_id, hidden, cell, encoder_outputs,\n",
    "                self_attn_mask[:, :, :t+2, :t+2] if t < max_len-1 else self_attn_mask\n",
    "            )\n",
    "            next_token = output.argmax(1).unsqueeze(1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "            input_id = next_token\n",
    "            if (next_token == eos_token).all():\n",
    "                break\n",
    "\n",
    "        return generated_ids"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyML9SG6XxTTtmQh4xsm+XVU",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037ef42c503745e8b511c3b67e19c33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0564f2cba4a1417ca19d80685a961fb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0724347077b4db191fa46fa2d9c8938",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c7ebf122eceb4962b2dd04102d9fce57",
      "value": 124
     }
    },
    "229594db80b5495bb57edeb247c51c3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "273fe7f8c42b4aa29bdc29c8636b3797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93f1bbe722ce4b7aa6da21eefdac502c",
      "placeholder": "​",
      "style": "IPY_MODEL_b9595532688548e3a10b40c712d8f0d7",
      "value": " 548M/548M [00:06&lt;00:00, 180MB/s]"
     }
    },
    "2a88af8ea15b4dd69a5ec53725a213cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8f05653ad7448b3ae8eabbc0a267445",
      "placeholder": "​",
      "style": "IPY_MODEL_f8d5c4d3b8744c22b284d22db9347083",
      "value": " 124/124 [00:00&lt;00:00, 8.86kB/s]"
     }
    },
    "37e46a4a9eca43cf87ab9f8281deb9c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "459fda7704ac44bf817a68edad821662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a56e78c60446ae99677c36d80b2b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da8f6cc646f04c349b1744c770bf01ef",
      "placeholder": "​",
      "style": "IPY_MODEL_a47399e92d3e434193e60eae45453845",
      "value": "model.safetensors: 100%"
     }
    },
    "5a31a52c17234878a5cfe017b1b1fe2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f5a9e7a78da4c04b8cc5e3c3912d8b2",
       "IPY_MODEL_0564f2cba4a1417ca19d80685a961fb6",
       "IPY_MODEL_2a88af8ea15b4dd69a5ec53725a213cc"
      ],
      "layout": "IPY_MODEL_229594db80b5495bb57edeb247c51c3b"
     }
    },
    "68d8705099ed4ebaa86d154c02d32916": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56a56e78c60446ae99677c36d80b2b4c",
       "IPY_MODEL_84aae215b4bc4d20abdf8a0f2d6a543f",
       "IPY_MODEL_273fe7f8c42b4aa29bdc29c8636b3797"
      ],
      "layout": "IPY_MODEL_9f9cc379e9ef4eca943e509757695662"
     }
    },
    "84aae215b4bc4d20abdf8a0f2d6a543f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_459fda7704ac44bf817a68edad821662",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37e46a4a9eca43cf87ab9f8281deb9c0",
      "value": 548105171
     }
    },
    "8f5a9e7a78da4c04b8cc5e3c3912d8b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e603e9110d2e44248f15c98c0f5b50c3",
      "placeholder": "​",
      "style": "IPY_MODEL_037ef42c503745e8b511c3b67e19c33a",
      "value": "generation_config.json: 100%"
     }
    },
    "93f1bbe722ce4b7aa6da21eefdac502c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f9cc379e9ef4eca943e509757695662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a47399e92d3e434193e60eae45453845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0724347077b4db191fa46fa2d9c8938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9595532688548e3a10b40c712d8f0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7ebf122eceb4962b2dd04102d9fce57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da8f6cc646f04c349b1744c770bf01ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e603e9110d2e44248f15c98c0f5b50c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8f05653ad7448b3ae8eabbc0a267445": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8d5c4d3b8744c22b284d22db9347083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
