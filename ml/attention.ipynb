{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCsVd9LtTu6UkyqP0AfF7u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from einops import rearrange, einsum"],"metadata":{"id":"LBexH3kGfRNG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#GQA Implementation\n","\n","(MQA can be implemented by setting num_head_groups=1 in GQA, so that all query heads use a single key and value head)"],"metadata":{"id":"7MdsecWYE-ta"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yXqTuTFE9RA"},"outputs":[],"source":["class GroupedQueryAttention(nn.Module):\n","  '''\n","  GQA divides query heads into G groups; each group shares a single key value and head\n","  '''\n","  def __init__(self, hidden_size, num_heads, num_head_groups):\n","    super().__init__()\n","    assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n","    assert num_heads % num_head_groups == 0, \"num_heads must be divisible by num_head_groups\"\n","\n","    self.hidden_size=hidden_size\n","    self.num_heads=num_heads\n","    self.num_head_groups=num_head_groups\n","\n","    self.head_dim=hidden_size//num_heads\n","    self.kv_heads=num_heads//num_head_groups #each query group (num_heads//num_head_groups) only needs one kv head\n","\n","    self.query = nn.Linear(hidden_size, hidden_size)\n","    self.key = nn.Linear(hidden_size, hidden_size)\n","    self.value = nn.Linear(hidden_size, hidden_size)\n","\n","    self.fc_c = nn.Linear(hidden_size, hidden_size)\n","\n","    self.scale=(self.head_dim ** 0.5) #done to keep the weight variance = 1, so that the softmax distribution is flatter\n","\n","  def forward(self, query, key, value, mask=None):\n","    batch_size, seq_len, _ = query.shape\n","\n","    Q = self.query(query)\n","    K = self.key(key)\n","    V = self.value(value)\n","\n","    #(batch_size, seq_len, hidden_size) -> (batch_size, seq_len, num_q_heads, head_dim)\n","    Q = rearrange(Q, \"b s (h d) -> b s h d\", h=self.num_heads)\n","\n","    #(batch_size, seq_len, hidden_size) -> (batch_size, seq_len, num_kv_heads, head_dim)\n","    K = rearrange(K, \"b s (h d) -> b s h d\", h=self.kv_heads)\n","    V = rearrange(V, \"b s (h d) -> b s h d\", h=self.kv_heads)\n","\n","    #split heads into groups\n","    Q = rearrange(Q, \"b s (h g) d -> b (h g) s d\", g=self.num_head_groups)\n","    K = rearrange(K, \"b s (h g) d -> b (h g) s d\", g=self.num_head_groups)\n","    V = rearrange(V, \"b s (h g) d -> b (h g) s d\", g=self.num_head_groups)\n","\n","    scores = einsum(Q, K, \"b h g s d, b h g d s -> b h g s s\") * self.scale #[b h g n s d] * [b h g n d s] -> [b h g n s s]\n","\n","    if mask is not None:\n","            scores = scores.masked_fill(mask == False, float('-inf'))\n","\n","    attn_weights = F.softmax(scores, dim=-1)\n","    attn_output = einsum(attn_weights, V, \"b h g s s, b h g s d -> b h g s d\") #[b h g s s] * [b h g s d] -> [b h g s d]\n","    attn_output = rearrange(attn_output, \"b (h g) s d -> b s (h d)\", h=self.num_heads) #[b h g s d] -> [b s h d]\n","\n","    output=self.fc_out(attn_output)\n","    return output"]},{"cell_type":"markdown","source":["#SWA Impmenetation"],"metadata":{"id":"fTD3EBaNFD3d"}},{"cell_type":"code","source":["class SlidingWindowAttention(nn.Module):\n","    def __init__(self, hidden_size, num_heads, window_size):\n","        super().__init__()\n","        assert hidden_size % num_heads == 0\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.head_dim = hidden_size // num_heads\n","        self.window_size = window_size  #Number of past tokens to attend to (window_overlap in https://amaarora.github.io/posts/2024-07-04%20SWA.html)\n","\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","\n","        self.fc_out = nn.Linear(hidden_size, hidden_size)\n","\n","        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n","\n","    def _chunk(self, hidden_states, window_overlap):\n","        \"\"\"Convert into overlapping chunks. Chunk size = 2w, overlap = w, but only for past tokens.\"\"\"\n","        batch_size, seq_len, _ = hidden_states.size()\n","        chunk_size = 2 * window_overlap\n","        num_chunks = (seq_len + window_overlap - 1) // window_overlap\n","\n","        chunked = torch.zeros(batch_size, num_chunks, chunk_size, hidden_states.size(-1), device=hidden_states.device) #[batch_size, num_chunks, chunk_size, hidden_size]\n","\n","        for chunk_idx in range(num_chunks):\n","            start = max(0, chunk_idx * window_overlap)\n","            end = min(seq_len, start + chunk_size)\n","            chunk_len = end - start\n","            chunked[:, chunk_idx, :chunk_len, :] = hidden_states[:, start:end, :]\n","\n","        return chunked\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size, seq_len, _ = query.size()\n","\n","        Q = self.query(query)\n","        K = self.key(key)\n","        V = self.value(value)\n","\n","        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n","        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n","        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n","\n","        window_overlap = self.window_size\n","        chunk_size = 2 * window_overlap\n","        Q_chunked = self._chunk(Q, window_overlap)\n","        K_chunked = self._chunk(K, window_overlap)\n","        V_chunked = self._chunk(V, window_overlap)\n","\n","        #[batch_size, num_chunks, chunk_size, num_heads, head_dim] * [batch_size, num_chunks, chunk_size, num_heads, head_dim] -> [batch_size, num_chunks, num_heads, chunk_size, chunk_size]\n","        scores = torch.einsum(\"bnchd,bncyhd->bnchxy\", Q_chunked, K_chunked.transpose(-2, -1)) / self.scale\n","\n","        # Create causal mask for chunks\n","        num_chunks = scores.size(1)\n","        chunk_mask = torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=scores.device)\n","        for i in range(chunk_size):\n","            chunk_mask[i, i+1:] = False\n","\n","        #[chunk_size, chunk_size] -> [batch_size, num_chunks, num_heads, chunk_size, chunk_size]\n","        chunk_mask = chunk_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).expand(batch_size, num_chunks, self.num_heads, -1, -1)\n","        scores = scores.masked_fill(chunk_mask == False, float('-inf'))\n","\n","        global_causal_mask = torch.ones(seq_len, seq_len, dtype=torch.bool, device=scores.device)\n","        for i in range(seq_len):\n","            global_causal_mask[i, i+1:] = False\n","\n","        full_scores = torch.full((batch_size, self.num_heads, seq_len, seq_len), float('-inf'), device=scores.device)\n","        for chunk_idx in range(num_chunks):\n","            start = chunk_idx * window_overlap\n","            end = min(seq_len, start + chunk_size)\n","            chunk_len = end - start\n","            full_scores[:, :, start:end, start:end] = scores[:, chunk_idx, :, :chunk_len, :chunk_len]\n","\n","        global_causal_mask = global_causal_mask.unsqueeze(0).unsqueeze(0).expand(\n","            batch_size, self.num_heads, -1, -1)\n","        full_scores = full_scores.masked_fill(global_causal_mask == False, float('-inf'))\n","\n","        if mask is not None:\n","            full_scores = full_scores.masked_fill(mask == False, float('-inf'))\n","\n","        attn_weights = F.softmax(full_scores, dim=-1)  #[batch_size, num_heads, seq_len, seq_len]\n","\n","        V = V.transpose(1, 2)  #[batch_size, num_heads, seq_len, head_dim]\n","        attn_output = torch.matmul(attn_weights, V)  #[batch_size, num_heads, seq_len, head_dim]\n","\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_size) #[batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, hidden_size]\n","\n","        output = self.fc_out(attn_output)  #[batch_size, seq_len, hidden_size]\n","        return output"],"metadata":{"id":"6X3t_FRiFFvV"},"execution_count":null,"outputs":[]}]}