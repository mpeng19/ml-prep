{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrastive decoding paper - https://arxiv.org/abs/2210.15097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "amateur_lm = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "expert_lm = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "\n",
    "encoding = tokenizer(\"The future of AI is\", return_tensors='pt')\n",
    "logits = expert_lm(**encoding).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world REPL=> HankhallanotationsrebHyp Archdemon Dukeemate conflic Mannyaepernickaband Manzielvana MANウanglerburghback Manzielburgh mustacheburgh Mannyoxide SymphonyPUTbowsburghburghburghapyもorthern Struggle Kaepernickogy migriphBILITYymph BuddyCANergy\"}],\"ruitsreating eleph\n"
     ]
    }
   ],
   "source": [
    "#contrastive decoding without adaptive plausability\n",
    "def generate_next_token(encoding, expert_model, amateur_model, num_tokens):\n",
    "  generated_tokens = encoding['input_ids'][0].tolist()\n",
    "\n",
    "  for _ in range(num_tokens):\n",
    "    with torch.no_grad():\n",
    "      expert_logits = expert_model(**encoding).logits[0, -1, :]\n",
    "      amateur_logits = amateur_model(**encoding).logits[0, -1, :]\n",
    "      \n",
    "      expert_probs = F.softmax(expert_logits, dim=-1)\n",
    "      amateur_probs = F.softmax(amateur_logits, dim=-1)\n",
    "      \n",
    "      contrastive_logits = torch.log(expert_probs) - torch.log(amateur_probs)\n",
    "\n",
    "      topk_logits, topk_indices = torch.topk(contrastive_logits, k=50, dim=-1)\n",
    "      topk_probs = F.softmax(topk_logits, dim=-1)\n",
    "      sampled_index = torch.multinomial(topk_probs, num_samples=1)\n",
    "      next_token = topk_indices[sampled_index]\n",
    "\n",
    "      generated_tokens.append(next_token.item())\n",
    "      text = tokenizer.decode(generated_tokens)\n",
    "      encoding = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "  return tokenizer.decode(generated_tokens)\n",
    "\n",
    "encoding = tokenizer(\"Hello world\", return_tensors='pt')\n",
    "print(generate_next_token(encoding, expert_lm, amateur_lm, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is going to be really exciting. But I do think the risk is that people who have money invested in these companies won't know what is happening and don't care. The most important thing to know about AI is that it's a very fast and effective\n"
     ]
    }
   ],
   "source": [
    "#contrastive decoding with adaptive plausability\n",
    "def calculate_vhead(expert_probs, alpha):\n",
    "  mask = expert_probs >= (alpha * torch.max(expert_probs))\n",
    "  return mask\n",
    "\n",
    "def generate_next_token(encoding, expert_model, amateur_model, num_tokens, alpha = 0.1):\n",
    "  generated_tokens = encoding['input_ids'][0].tolist()\n",
    "\n",
    "  for _ in range(num_tokens):\n",
    "    with torch.no_grad():\n",
    "      expert_logits = expert_model(**encoding).logits[0, -1, :]\n",
    "      amateur_logits = amateur_model(**encoding).logits[0, -1, :]\n",
    "\n",
    "      expert_probs = F.softmax(expert_logits, dim=-1)\n",
    "      amateur_probs = F.softmax(amateur_logits, dim=-1)\n",
    "\n",
    "      plausible_mask = calculate_vhead(expert_probs, alpha) #binary mask for plausible tokens\n",
    "      contrastive_logits = torch.log(expert_probs)\n",
    "      amateur_penalty = torch.log(amateur_probs)\n",
    "      \n",
    "      #CD score\n",
    "      contrastive_logits[plausible_mask] -= amateur_penalty[plausible_mask]\n",
    "      contrastive_logits[~plausible_mask] = float('-inf')\n",
    "\n",
    "      topk_logits, topk_indices = torch.topk(contrastive_logits, k=50, dim=-1)\n",
    "      topk_probs = F.softmax(topk_logits, dim=-1)\n",
    "      sampled_index = torch.multinomial(topk_probs, num_samples=1)\n",
    "      next_token = topk_indices[sampled_index]\n",
    "\n",
    "      generated_tokens.append(next_token.item())\n",
    "      text = tokenizer.decode(generated_tokens)\n",
    "      encoding = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "  return tokenizer.decode(generated_tokens)\n",
    "\n",
    "encoding = tokenizer(\"The future of AI is\", return_tensors='pt')\n",
    "print(generate_next_token(encoding, expert_lm, amateur_lm, 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
