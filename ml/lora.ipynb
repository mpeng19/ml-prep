{"cells":[{"cell_type":"markdown","source":["Implementation of LORA (low rank adaptation) - as presented in https://r4j4n github.io/blogs/posts/lora/. Original paper at https://arxiv.org/abs/2106.09685."],"metadata":{"id":"b3E_h1WIfJD9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Q1ekmBFwHtx"},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2744,"status":"ok","timestamp":1747379609530,"user":{"displayName":"Michael Peng","userId":"09920448190389534354"},"user_tz":240},"id":"eOzI37plwL0_","outputId":"373e3781-fb4a-4827-a1d6-e34d98eca1b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1001, 1000]) torch.Size([10000, 1001]) torch.Size([10000, 1000])\n"]}],"source":["n=10_000\n","d_in = 1001\n","d_out=1000\n","hidden_dim=1000\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","theta = torch.randn(d_in, d_out).to(device)\n","X = torch.randn(n, d_in).to(device)\n","y = torch.matmul(X, theta).to(device)\n","\n","print(theta.shape, X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161667,"status":"ok","timestamp":1747379778653,"user":{"displayName":"Michael Peng","userId":"09920448190389534354"},"user_tz":240},"id":"Od0zvfHVwvt7","outputId":"965b5b22-6bb9-4530-a164-a59fefe54d51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0/50 | Loss:864.793212890625\n","Epoch: 10/50 | Loss:18.980276107788086\n","Epoch: 20/50 | Loss:1.2586461305618286\n","Epoch: 30/50 | Loss:0.15425437688827515\n","Epoch: 40/50 | Loss:0.02819485031068325\n"]}],"source":["class LinRegModel(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, output_dim):\n","    super().__init__()\n","    self.layer1=nn.Linear(input_dim, hidden_dim, bias=False)\n","    self.layer2=nn.Linear(hidden_dim, output_dim, bias=False)\n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    return out\n","\n","\n","def train(model, X, y, batch_size=128, epochs=50):\n","  opt = torch.optim.Adam(model.parameters())\n","\n","  for epoch in range(epochs):\n","    permutation = torch.randperm(X.size()[0])\n","\n","    for i in range(0, X.size()[0], batch_size):\n","      opt.zero_grad()\n","      indices = permutation[i:i+batch_size]\n","      batch_x, batch_y = X[indices], y[indices]\n","      outputs=model(batch_x)\n","      loss=torch.nn.functional.mse_loss(outputs, batch_y)\n","      loss.backward()\n","      opt.step()\n","\n","    if epoch%10==0:\n","      with torch.no_grad():\n","        outputs=model(X)\n","        loss=torch.nn.functional.mse_loss(outputs, y)\n","        print(f\"Epoch: {epoch}/{epochs} | Loss:{loss.item()}\")\n","\n","model = LinRegModel(d_in, hidden_dim, d_out).to(device)\n","train(model, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1747379938966,"user":{"displayName":"Michael Peng","userId":"09920448190389534354"},"user_tz":240},"id":"0b7ayNhoyLOS","outputId":"b4417a95-5bda-434c-b1e1-c6cf03e53f30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss on different distribution: 1009.7973022460938\n"]}],"source":["#draw data from different distribution (add 1 to original distribution)\n","#data is \"low signal\", ie we do not need to train on 10k examples to learn this shift\n","#lora will allow us to train on a \"dumbed down\" version of this data; this should be enough to learn\n","theta2 = theta + 1\n","X2 = torch.randn(n, d_in).to(device)\n","y2 = torch.matmul(X2, theta2).to(device)\n","\n","\n","#try our new data with our old model\n","loss = torch.nn.functional.mse_loss(model(X2), y2)\n","print(f\"Loss on different distribution: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtsvzYJ-5eKl"},"outputs":[],"source":["class AdaptedLinear(nn.Module):\n","    def __init__(self, linear, r, scaling ) -> None:\n","        super().__init__()\n","        linear.requires_grad_(False)\n","        self.linear = linear\n","        self.A = nn.Parameter(torch.randn(linear.in_features, r)) #(r,d)\n","        self.B = nn.Parameter(torch.zeros(r, linear.out_features)) #(d,r)\n","        self.scaling = scaling\n","\n","    def forward(self, x):\n","        return self.linear(x) + torch.matmul(x, torch.matmul(self.A, self.B) * self.scaling) #lora update Wx + BAx * scaling factor (alpha/r)\n","\n","\n","class LoraAdapter(nn.Module):\n","  def __init__(self, model, r=16, alpha=1):\n","    super().__init__()\n","    self.module_list=nn.ModuleList()\n","    self.scaling=alpha/r #learning rate\n","    self.original_linears=[]\n","\n","    #go through layers of model and add adapters to all linear layers\n","    for layer in model.children():\n","      if isinstance(layer, nn.Linear):\n","        #keep reference to original layer\n","        self.original_linears.append(layer)\n","        adapted_layer=AdaptedLinear(layer, r, self.scaling)\n","        self.module_list.append(adapted_layer)\n","      else:\n","        self.module_list.append(layer)\n","\n","\n","  def forward(self, x):\n","    for layer in self.module_list:\n","      x=layer(x)\n","    return x\n","\n","  def update_original_weights(self):\n","    with torch.no_grad():\n","      for adapted_layer, original_layer in zip(self.module_list, self.original_linears):\n","        delta_theta = torch.matmul(adapted_layer.A, adapted_layer.B) * adapted_layer.scaling\n","        original_layer.weight.add_(delta_theta.t())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3ivZ7kDm7lu5","outputId":"e303fbad-8130-4031-bb6c-70f64917340d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0/100 | Loss:1003.4911499023438\n","Epoch: 10/100 | Loss:684.5513916015625\n","Epoch: 20/100 | Loss:307.9579162597656\n","Epoch: 30/100 | Loss:109.58228302001953\n","Epoch: 40/100 | Loss:32.0892333984375\n","Epoch: 50/100 | Loss:7.14031457901001\n","Epoch: 60/100 | Loss:1.1468110084533691\n","Epoch: 70/100 | Loss:0.2185971438884735\n","Epoch: 80/100 | Loss:0.10957492142915726\n","Epoch: 90/100 | Loss:0.08853045850992203\n"]}],"source":["lora_model=LoraAdapter(model, r=1).to(device)\n","train(lora_model, X=X2,y=y2, epochs=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnBqZA4i7ofl"},"outputs":[],"source":["loss = torch.nn.functional.mse_loss(model(X2), y2)\n","print(f\"Loss on different distribution: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jfj6uNOp7usq"},"outputs":[],"source":["lora_model.update_original_weights()\n","loss = torch.nn.functional.mse_loss(model(X2), y2)\n","print(f\"Loss on different distribution: {loss}\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEFIks65Jqzr7fwoG8sR+W"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}